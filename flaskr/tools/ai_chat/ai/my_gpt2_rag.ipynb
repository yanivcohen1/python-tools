{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc135a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ---------- Custom Output Class ----------\n",
    "class QuestionAnsweringModelOutput:\n",
    "    def __init__(self, loss=None, start_logits=None, end_logits=None, hidden_states=None, attentions=None, reasoning_logits=None):\n",
    "        self.loss = loss\n",
    "        self.start_logits = start_logits\n",
    "        self.end_logits = end_logits\n",
    "        self.hidden_states = hidden_states\n",
    "        self.attentions = attentions\n",
    "        self.reasoning_logits = reasoning_logits\n",
    "\n",
    "# ---------- Custom BERT Implementation ----------\n",
    "class BertConfigCustom:\n",
    "    def __init__(self,\n",
    "                 vocab_size=30522,\n",
    "                 hidden_size=768,\n",
    "                 num_hidden_layers=12,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=2,\n",
    "                 reasoning_vocab_size=3):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.reasoning_vocab_size = reasoning_vocab_size\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words = self.word_embeddings(input_ids)\n",
    "        positions = self.position_embeddings(position_ids)\n",
    "        device = self.token_type_embeddings.weight.device\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        types = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words + positions + types\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        return self.dropout(embeddings)\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\"Hidden size must be divisible by number of heads\")\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // self.num_heads\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key   = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        def transpose(x):\n",
    "            return x.view(batch_size, seq_len, self.num_heads, self.head_dim) \\\n",
    "                    .permute(0, 2, 1, 3)\n",
    "\n",
    "        Q = transpose(self.query(hidden_states))\n",
    "        K = transpose(self.key(hidden_states))\n",
    "        V = transpose(self.value(hidden_states))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        scores = scores / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "        probs = self.dropout(probs)\n",
    "\n",
    "        context = torch.matmul(probs, V)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()\n",
    "        return context.view(batch_size, seq_len, -1)\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden = self.dense(hidden_states)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.LayerNorm(hidden + input_tensor)\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        self_output = self.self(hidden_states, attention_mask)\n",
    "        return self.output(self_output, hidden_states)\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return self.intermediate_act_fn(self.dense(hidden_states))\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden = self.dense(hidden_states)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.LayerNorm(hidden + input_tensor)\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        return self.output(intermediate_output, attention_output)\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        all_hidden = []\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "            all_hidden.append(hidden_states)\n",
    "        return hidden_states, all_hidden\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        cls_token = hidden_states[:, 0]\n",
    "        return self.activation(self.dense(cls_token))\n",
    "\n",
    "class BertModelCustom(nn.Module):\n",
    "    def __init__(self, config: BertConfigCustom):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        if attention_mask is not None:\n",
    "            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_mask = (1.0 - extended_mask) * -10000.0\n",
    "        else:\n",
    "            extended_mask = None\n",
    "\n",
    "        emb = self.embeddings(input_ids, token_type_ids)\n",
    "        seq_out, all_hidden = self.encoder(emb, extended_mask)\n",
    "        pooled = self.pooler(seq_out)\n",
    "        return seq_out, pooled, all_hidden\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapper for SQuAD-style QA data.\n",
    "    Expects data as a list of dicts: {\"context\": ..., \"question\": ..., \"answers\": {...}}\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=384, doc_stride=128):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.doc_stride = doc_stride\n",
    "\n",
    "        for entry in data:\n",
    "            inputs = tokenizer(\n",
    "                entry['question'], entry['context'],\n",
    "                truncation=\"only_second\",\n",
    "                max_length=self.max_length,\n",
    "                stride=self.doc_stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            for i, offset in enumerate(inputs['offset_mapping']):\n",
    "                sample = {\n",
    "                    'input_ids': torch.tensor(inputs['input_ids'][i]),\n",
    "                    'attention_mask': torch.tensor(inputs['attention_mask'][i]),\n",
    "                }\n",
    "                answer = entry['answers']['text'][0]\n",
    "                start_char = entry['answers']['answer_start'][0]\n",
    "                end_char = start_char + len(answer)\n",
    "\n",
    "                sequence_ids = inputs.sequence_ids(i)\n",
    "                token_start = 0\n",
    "                while sequence_ids[token_start] != 1:\n",
    "                    token_start += 1\n",
    "                token_end = len(inputs['input_ids'][i]) - 1\n",
    "                while sequence_ids[token_end] != 1:\n",
    "                    token_end -= 1\n",
    "\n",
    "                if not (offset[token_start][0] <= start_char and offset[token_end][1] >= end_char):\n",
    "                    sample['start_positions'] = torch.tensor(0)\n",
    "                    sample['end_positions'] = torch.tensor(0)\n",
    "                else:\n",
    "                    while token_start < len(offset) and offset[token_start][0] <= start_char:\n",
    "                        token_start += 1\n",
    "                    sample['start_positions'] = torch.tensor(token_start - 1)\n",
    "                    while token_end >= 0 and offset[token_end][1] >= end_char:\n",
    "                        token_end -= 1\n",
    "                    sample['end_positions'] = torch.tensor(token_end + 1)\n",
    "\n",
    "                self.examples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "class BertForQuestionAnsweringCustom(nn.Module):\n",
    "    def __init__(self, config: BertConfigCustom):\n",
    "        super().__init__()\n",
    "        self.bert = BertModelCustom(config)\n",
    "        hidden_size = config.hidden_size\n",
    "        self.qa_outputs = nn.Linear(hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                start_positions=None,\n",
    "                end_positions=None):\n",
    "        seq_out, _, _ = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        sequence_output = self.dropout(seq_out)\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits   = end_logits.squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_start = loss_fct(start_logits, start_positions)\n",
    "            loss_end   = loss_fct(end_logits,   end_positions)\n",
    "            loss = (loss_start + loss_end) / 2\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "# ---------- Training & Evaluation Continued ----------\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ---------- Inference Function ----------\n",
    "def evaluate(model, tokenizer, question, context, device):\n",
    "    model.eval()\n",
    "    enc = tokenizer(question, context, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "    for k in enc: enc[k] = enc[k].to(device)\n",
    "    outputs = model(enc['input_ids'], enc['attention_mask'], enc.get('token_type_ids', None))\n",
    "    start = torch.argmax(outputs.start_logits, dim=1).item()\n",
    "    end = torch.argmax(outputs.end_logits, dim=1).item() + 1\n",
    "    answer = tokenizer.decode(enc['input_ids'][0][start:end])\n",
    "    return answer\n",
    "\n",
    "# ---------- Chain-of-Thought Reasoning Modification ----------\n",
    "class BertForQuestionAnsweringWithReasoning(nn.Module):\n",
    "    def __init__(self, config: BertConfigCustom):\n",
    "        super().__init__()\n",
    "        self.bert = BertModelCustom(config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.reasoning_output = nn.Linear(config.hidden_size, config.reasoning_vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
    "                start_positions=None, end_positions=None, reasoning_labels=None):\n",
    "        seq_out, hidden_states, attentions = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        logits = self.qa_outputs(seq_out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        reasoning_logits = self.reasoning_output(seq_out[:, 0])  # [CLS]\n",
    "\n",
    "        loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            loss = (start_loss + end_loss) / 2\n",
    "            if reasoning_labels is not None:\n",
    "                reason_loss = loss_fct(reasoning_logits, reasoning_labels)\n",
    "                loss = loss + reason_loss\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=hidden_states,\n",
    "            attentions=attentions,\n",
    "            reasoning_logits=reasoning_logits\n",
    "        )\n",
    "\n",
    "# ---------- Dataset Class with Reasoning Support ----------\n",
    "class QADatasetWithReasoning(Dataset):\n",
    "    def __init__(self, data, tokenizer, reasoning_vocab, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reasoning_vocab = reasoning_vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        answer = item['answers']['text'][0]\n",
    "        start_char = item['answers']['answer_start'][0]\n",
    "        end_char = start_char + len(answer)\n",
    "        reasoning_label = self.reasoning_vocab.get(item.get('reasoning_label', 'factual'), 0)\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            question, context,\n",
    "            truncation='only_second',\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        offsets = enc.pop('offset_mapping')[0]\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        token_type_ids = enc.get('token_type_ids', None)\n",
    "        if token_type_ids is not None: token_type_ids = token_type_ids.squeeze(0)\n",
    "\n",
    "        start_pos = end_pos = 0\n",
    "        for i, (s, e) in enumerate(offsets.tolist()):\n",
    "            if s <= start_char < e: start_pos = i\n",
    "            if s < end_char <= e:\n",
    "                end_pos = i\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'start_positions': torch.tensor(start_pos),\n",
    "            'end_positions': torch.tensor(end_pos),\n",
    "            'reasoning_labels': torch.tensor(reasoning_label)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Q&A dataset from a PDF file\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# Load data\n",
    "import fitz\n",
    "import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Extract text\n",
    "doc = fitz.open(current_dir + '/datasets/alice.pdf')\n",
    "text = ''.join([p.get_text() for p in doc])\n",
    "sentences = sent_tokenize(text)\n",
    "passages = [' '.join(sentences[i:i+5]) for i in range(0, len(sentences), 5)]\n",
    "\n",
    "# Build QA examples\n",
    "qa_examples = []\n",
    "for i, p in enumerate(passages[:50]):\n",
    "    question = f\"What is the main idea in passage {i}?\"\n",
    "    answer = sentences[i*5] if i*5 < len(sentences) else ''\n",
    "    start_idx = p.find(answer)\n",
    "    qa_examples.append({\n",
    "        'context': p,\n",
    "        'question': question,\n",
    "        'answers': {'text': [answer], 'answer_start': [start_idx]},\n",
    "        'reasoning_label': 'factual'\n",
    "    })\n",
    "\n",
    "# Save\n",
    "with open(current_dir + '/datasets/alice_qa.json', 'w') as f:\n",
    "    json.dump(qa_examples, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0da61ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load Q&A and prepare\n",
    "with open('datasets/alice_qa.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "vocab = {'factual': 0, 'causal': 1, 'comparative': 2}\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "dataset = QADatasetWithReasoning(data, tokenizer, vocab)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = BertConfigCustom(reasoning_vocab_size=len(vocab))\n",
    "model = BertForQuestionAnsweringWithReasoning(config).to(device)\n",
    "opt = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "print(\"devices:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 1 with loss 0.0228\n",
      "Epoch 1: loss = 0.0228\n",
      "Epoch 2: loss = 0.0379\n",
      "Epoch 3: loss = 0.0384\n",
      "Model saved at epoch 4 with loss 0.0120\n",
      "Epoch 4: loss = 0.0120\n",
      "Epoch 5: loss = 0.0239\n",
      "Epoch 6: loss = 0.0165\n",
      "Epoch 7: loss = 0.0264\n",
      "Epoch 8: loss = 0.0418\n",
      "Epoch 9: loss = 0.0257\n",
      "Epoch 10: loss = 0.0544\n",
      "Epoch 11: loss = 0.0341\n",
      "Epoch 12: loss = 0.0213\n",
      "Epoch 13: loss = 0.0373\n",
      "Epoch 14: loss = 0.0270\n",
      "Epoch 15: loss = 0.0464\n",
      "Epoch 16: loss = 0.0232\n",
      "Epoch 17: loss = 0.0305\n",
      "Epoch 18: loss = 0.0231\n",
      "Epoch 19: loss = 0.0143\n",
      "Epoch 20: loss = 0.0304\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        opt.zero_grad()\n",
    "        out = model(\n",
    "            batch['input_ids'].to(device),\n",
    "            batch['attention_mask'].to(device),\n",
    "            batch.get('token_type_ids', None),\n",
    "            batch['start_positions'].to(device),\n",
    "            batch['end_positions'].to(device),\n",
    "            batch['reasoning_labels'].to(device)\n",
    "        )\n",
    "        out.loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += out.loss.item()\n",
    "    print(f'Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bf8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "os.makedirs(\"models/qa_model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/my_gpt2_rag_model.pth\") # model.state_dict()\n",
    "tokenizer.save_pretrained(\"models/qa_model\")\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c16bc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnsweringWithReasoning(\n",
       "  (bert): BertModelCustom(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELU(approximate='none')\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (reasoning_output): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model for evaluation\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"models/qa_model\")\n",
    "model = BertForQuestionAnsweringWithReasoning(config)\n",
    "model.load_state_dict(torch.load(\"models/my_gpt2_rag_model.pth\"))\n",
    "model.to(device)\n",
    "# model.load_state_dict(torch.load(\"models/my_gpt3_model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0e51fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the main idea in passage 10?\n",
      "A: what is the main idea in passage 10? [SEP] thump! down she came upon a heap of sticks and dry leaves, and the fall was over.\n",
      "E: thump!\n",
      " --------------------------------------------------\n",
      "Q: What is the main idea in passage 46?\n",
      "A: “ he took me for his housemaid, ” she said to herself as she ran.\n",
      "E: “He took me for his housemaid,” she said to herself as she ran.\n",
      " --------------------------------------------------\n",
      "Q: What is the main idea in passage 4?\n",
      "A: in another moment down went alice after it, never once considering how in the world she was to get out again.\n",
      "E: In another moment down went Alice after it, never once considering how\n",
      "in the world she was to get out again.\n",
      " --------------------------------------------------\n",
      "Q: What is the main idea in passage 11?\n",
      "A: suddenly she came upon a little three - legged table, all made of solid glass ; there was nothing on it except a tiny golden key, and alice ’ s first thought was that it might belong to one of the doors of the hall ; but, alas!\n",
      "E: Suddenly she came upon a little three-legged table, all made of solid\n",
      "glass; there was nothing on it except a tiny golden key, and Alice’s\n",
      "first thought was that it might belong to one of the doors of the \n",
      "hall;\n",
      "but, alas!\n",
      " --------------------------------------------------\n",
      "Q: What is the main idea in passage 26?\n",
      "A: “ i wish i hadn ’ t cried so much! ” said alice, as she swam about, trying to find her way out.\n",
      "E: “I wish I hadn’t cried so much!” said Alice, as she swam about, trying\n",
      "to find her way out.\n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# evaluate the reasoning model\n",
    "import random\n",
    "for i in range(5):\n",
    "    idx = random.randint(0, len(data) - 1)\n",
    "    q = data[idx]['question']\n",
    "    c = data[idx]['context']\n",
    "    expected = data[idx]['answers']['text'][0]\n",
    "    ans = evaluate(model, tokenizer, q, c, device)\n",
    "    print(f\"Q: {q}\\nA: {ans}\\nE: {expected}\\n {'-'*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
