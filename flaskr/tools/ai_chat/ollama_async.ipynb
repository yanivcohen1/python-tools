{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You need start ollama server first in cmd run (ollama serve) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deepseek-r1:14b', 'deepseek-r1:32b', 'deepseek-r1:8b']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://localhost:11434/v1/models\")\n",
    "response.raise_for_status()\n",
    "data = response.json()\n",
    "models = data['data']\n",
    "models_id = [model['id'] for model in models]\n",
    "models_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "sw = widgets.Select(\n",
    "    options=models_id,\n",
    "    value=models_id[0],\n",
    "    rows=10,\n",
    "    description='please select model:',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, please select model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed48c4baa144a31a77114bacf485159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(description='please select model:', options=('deepseek-r1:14b', 'deepseek-r1:32b', 'deepseek-r1:8b'), r…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4f2fbd80ed4e3d824ef53f195bc344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='continue', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cffcf8976af4aa7886647707d119532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "output = widgets.Output()\n",
    "\n",
    "@output.capture(clear_output=False,wait=True) # based on https://github.com/jupyter-widgets/ipywidgets/issues/1846 and https://ipywidgets.readthedocs.io/en/latest/examples/Output%20Widget.html\n",
    "def sayHello(b):\n",
    "    print(\"selected:\", sw.value)\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description = 'continue'\n",
    ")\n",
    "print(\"Hello, please select model\")\n",
    "run_button.on_click(sayHello)\n",
    "display(sw)\n",
    "display(run_button)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai aiohttp nest_asyncio\n",
    "\n",
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "import textwrap\n",
    "import asyncio\n",
    "import openai\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "openai.api_key = 'lm-studio'\n",
    "\n",
    "code_highlight = \", the code in response will be like this ```{program_language_name}\"\n",
    "\n",
    "def to_markdown(text):\n",
    "    text = text.replace('•', '  *')\n",
    "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "async def fetch_completion(my_msg, code_highlights = False):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(\n",
    "            'http://localhost:11434/v1/chat/completions',\n",
    "            headers={\n",
    "                # 'Authorization': f'Bearer {openai.api_key}',\n",
    "                'Content-Type': 'application/json'\n",
    "            },\n",
    "            json={\n",
    "                'model': str(sw.value),\n",
    "                \"messages\": [\n",
    "                  { \"role\": \"system\", \"content\": \"if code is in response add the code like this ```{program_language_name}\" },\n",
    "                  { \"role\": \"user\", \"content\": my_msg if not code_highlights else my_msg + code_highlight }\n",
    "                ],\n",
    "                'temperature': 0.7,\n",
    "                \"max_tokens\": -1, # -1 for unlimited\n",
    "                \"stream\": True\n",
    "            }\n",
    "        ) as response:\n",
    "            content = \"\"\n",
    "            display_handle = display(to_markdown(content), display_id=True)\n",
    "            async for line in response.content:\n",
    "                line = line.decode('utf-8').strip()\n",
    "                if line.startswith(\"data: \"):\n",
    "                    data = line[len(\"data: \"):]\n",
    "                    if data != '[DONE]':\n",
    "                        json_data = json.loads(data)\n",
    "                        delta = json_data['choices'][0]['delta']\n",
    "                        if 'content' in delta:\n",
    "                            content += delta['content']\n",
    "                            display_handle.update(to_markdown(content))\n",
    "            return content\n",
    "\n",
    "# prompt = \"Once upon a time\"\n",
    "# full_ans = await fetch_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> <think>\n",
       "> Okay, I need to write a Python function called scrape_url_data. The goal is to extract data from a URL when given as input. Hmm, first, I should figure out how URLs are structured. URLs typically have different parts like protocol (http, https), domain, path, query parameters, and fragment.\n",
       "> \n",
       "> I remember that the urllib library in Python has modules that can help parse URLs. Oh right, there's urlparse from urllib.parse. So I'll probably use that to break down the URL into its components.\n",
       "> \n",
       "> Let me outline what each part means: scheme is http or https, netloc is the domain, path is the part after the domain, query is the parameters after the '?', fragment is anything after '#'. The function should return these parts as a dictionary with keys like 'scheme', 'netloc', 'path', 'query', and 'fragment'.\n",
       "> \n",
       "> I'll start by importing urlparse from urllib.parse. Then, inside the function, I'll take the URL input. Next, use urlparse(url) to get the components. Wait, but sometimes URLs might not be parsed correctly if they're malformed. But since this is a basic example, I can assume the URL is valid.\n",
       "> \n",
       "> I should handle cases where some parts are empty or None. For instance, the scheme and netloc can't be empty. So maybe include checks for that, but perhaps it's beyond the scope of this simple function.\n",
       "> \n",
       "> Now, construct a dictionary with each component. Make sure to convert the path into a string in case it's already a bytes object. Also, handle any potential encoding issues, but again, maybe not necessary here.\n",
       "> \n",
       "> Testing an example: if the URL is 'https://www.example.com/path?query=123#fragment', the function should return scheme='https', netloc='www.example.com', path='/path', query='query=123', fragment='fragment'.\n",
       "> \n",
       "> I think that's it. Now, write the code step by step, making sure each part is addressed.\n",
       "> </think>\n",
       "> \n",
       "> To extract URL data using Python, we can utilize the `urlparse` function from the `urllib.parse` module. Here's a concise example:\n",
       "> \n",
       "> ```python\n",
       "> from urllib.parse import urlparse\n",
       "> \n",
       "> def scrape_url_data(url):\n",
       ">     url_components = urlparse(url)\n",
       ">     return {\n",
       ">         'scheme': url_components.scheme,\n",
       ">         'netloc': url_components.netloc,\n",
       ">         'path': str(url_components.path),\n",
       ">         'query': url_components.query,\n",
       ">         'fragment': url_components.fragment\n",
       ">     }\n",
       "> ```\n",
       "> \n",
       "> This function breaks down a given URL into its components and returns them as a dictionary. Each key corresponds to a specific part of the URL, making it easy to analyze or process individual elements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"write in max 100 words python code that get url data\"\n",
    "full_ans = await fetch_completion(prompt, code_highlights=True)\n",
    "# print(full_ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
